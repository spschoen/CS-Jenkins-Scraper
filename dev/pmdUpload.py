"""
Reads pmd xml file uploads any new records into given database.

Requirements:
    Scraper.py  - library for interaction with databases must be available in the same directory as this file.
    config.json - file specifying database information.
    pmd.xml     - Obviously.  XML files generated by PMD.  If it doesn't exist, script exits.

Args:
    1. WORKSPACE  - Absolute path to the location of the findbugs.xml file
    2. PROJECT_ID - 17 char string representing class, section, project, and unique ID of the current project.
                    For example: csc216-002-P2-096
    3. GIT_COMMIT - 40 Character commit hash.

Returns:
    N/A

Authors:
    Renata Ann Zeitler
    Samuel Schoeneberger
"""

import xml.dom.minidom
import sys
import pymysql
import Scraper
import os
import platform

######################################################################
# Setup Section
######################################################################

FILE_DIR = Scraper.get_file_dir(sys.argv[1])

if platform.system() is "Windows":
    FILE_DIR += "\\"
else:
    FILE_DIR += "/"

if not (os.path.isfile(FILE_DIR + "pmd.xml")):
    print("Could not access " + FILE_DIR + "pmd.xml" + " Exiting.")
    sys.exit()

try:
    pmd = xml.dom.minidom.parse(FILE_DIR + 'pmd.xml')
except:
    # This is commented out, because pmd XML can be not created for a lot of reasons.
    print("Could not access pmd.xml file, but it exists.")
    sys.exit()

# Getting commitUID info
repo_id = sys.argv[2]
commit_hash = sys.argv[3]

# root is the first <> element in the XML file.
root = pmd.documentElement

# Set up to read XML

# Getting config options.
config_info = Scraper.get_config_options()
connection = pymysql.connect(host=config_info['ip'], user=config_info['user'],
                             password=config_info['pass'], db=config_info['db'])
cur = connection.cursor()

# CommitUID getting
commit_uid = Scraper.get_commit_uid(ip=config_info['ip'], user=config_info['user'], pw=config_info['pass'],
                                    db=config_info['db'], commit_hash=commit_hash, repo_id=repo_id)

######################################################################
# Setup complete
######################################################################

######################################################################
# Reading XML
######################################################################

# A basic for loop, to look at all the nodes (<> elements) inside the file node
# (which is now the root node) and print out their information to the DB.
# .childNodes is a list of nodes that the root has as children.
for file in pmd.getElementsByTagName("file"):
    for node in pmd.getElementsByTagName("violation"):

        package = ""
        class_name = ""
        method = ""
        line = 0
        rule = ""
        ruleset = ""

        if node.hasAttribute("beginline"):
            line = int(node.getAttribute("beginline"))
        if node.hasAttribute("rule"):
            rule = node.getAttribute("rule")
        if node.hasAttribute("ruleset"):
            ruleset = node.getAttribute("ruleset")
        if node.hasAttribute("package"):
            package = node.getAttribute("package").split('.')[-1]
        if node.hasAttribute("class"):
            class_name = node.getAttribute("class")
        if node.hasAttribute("method"):
            method = node.getAttribute("method")

        # holy FRAK it fits on the 100 limit!
        if package == "" or class_name == "" or method == "" or rule == "" or ruleset == "" or line == 0:
            # print("Could not find an attribute.  Rerun with print debugging.")
            continue

        # Class UID
        methodUID = Scraper.get_method_uid(ip=config_info['ip'], user=config_info['user'], pw=config_info['pass'],
                                           db=config_info['db'], package=package, class_name=class_name, method=method)

        search = "SELECT * FROM PMD WHERE CommitUID = %s AND MethodUID = %s AND Ruleset = %s AND " \
                 "Rule = %s AND Line = %s"
        cur.execute(search, (str(commit_uid), str(methodUID), str(ruleset), str(rule), str(line)))

        if cur.rowcount != 0:
            continue

        # PMD time!
        insertPMD = "INSERT INTO PMD(CommitUID, MethodUID, Ruleset, Rule, Line) VALUES ( %s, %s, %s, %s, %s )"

        try:
            cur.execute(insertPMD, (str(commit_uid), str(methodUID), str(ruleset), str(rule), str(line)))
        except:
            connection.rollback()
            error_string = str(sys.exc_info()[0]) + "\n----------\n"
            error_string += str(sys.exc_info()[1]) + "\n----------\n"
            error_string += str(sys.exc_info()[2])

            v_list = "(CommitUID, MethodUID, Ruleset, Rule, Line)"
            Scraper.sendFailEmail("Failed to insert into PMD table!", "The following insert failed:", insertPMD, v_list,
                                  error_string, str(commit_uid), str(methodUID), str(ruleset), str(rule), str(line))

# Closing connection
connection.close()
